{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fd7053",
   "metadata": {},
   "source": [
    "# Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef7291",
   "metadata": {},
   "source": [
    "## n-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fcbb69",
   "metadata": {},
   "source": [
    "### Probability of a Sentence\n",
    "* Naive Bayes model can compute the probability of a sentence, but is not a sophisticated model because it does not take into account interdependency of words on each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b90ffb",
   "metadata": {},
   "source": [
    "### Human Word Prediction\n",
    "* Requires domain knowledge, syntactic knowledge, lexical knowledge\n",
    "* Used by text generation, autocomplete, GPT technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee2689",
   "metadata": {},
   "source": [
    "### Probability of the Next Word\n",
    "* Instead of relying on domain, syntactic, and lexical knowledge, rely on the notion of **probability of a sequence**\n",
    "* Probability of the sequence $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$\n",
    "        P(w_n | w_1, w_2, ..., w_{n - 1})\n",
    "$$\n",
    "\n",
    "* Can generate the probability of the entire sequence using the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eff6c6",
   "metadata": {},
   "source": [
    "### Markov Assumption\n",
    "* Probability of a word $w_n$ given a sequence $w_1, ..., w_{n - 1}$ is difficult to estimate\n",
    "* The longer a sequence becomes, the less likely it will appear in the training data\n",
    "* Instead, make Markov assumptions regarding independence:\n",
    "    * The probability of $w_n$ only depends on the previous k-1 words\n",
    "\n",
    "$$ P(w_n | w_1, w_2, ..., w_{n - 1}) \\approx P(w_n | w_{n - k + 1}, ..., w_{n - 1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313985d8",
   "metadata": {},
   "source": [
    "### Bi-gram Language Model\n",
    "* Using the Markov assumption and the Chain Rule:\n",
    "\n",
    "$$ P(w_1, ..., w_n) \\approx P(w_1 | start) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdot \\cdot \\cdot P(w_n | w_{n -1}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2db08",
   "metadata": {},
   "source": [
    "### Log Probabilities\n",
    "* Probabilities can become very small, so we often work with log probabilities in practice\n",
    "\n",
    "$$ p(w_1, ..., w_n) = \\prod_{i = 1}^n p(w_i | w_{i - 1}) \\\\\n",
    "\\log p(w_1, ..., w_n) = \\sum_{i = 1}^n \\log p(w_i | w_{i - 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91095703",
   "metadata": {},
   "source": [
    "### Estimating n-gram Probabilities\n",
    "* We can estimate n-gram probabilities using maximum likelihood estimates\n",
    "\n",
    "$$ p(w | u) = \\frac{count(u, w)}{count(u)} $$\n",
    "\n",
    "* Or for trigrams:\n",
    "\n",
    "$$ p(w | u, v) = \\frac{count(u, v, w)}{count(u, v)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b069a39",
   "metadata": {},
   "source": [
    "### Unseen Tokens\n",
    "* If tokens are unseen, then counts become 0 in the denominator\n",
    "* The problem can be solved using the following approach:\n",
    "    * Start wtih a specific lexicon of known tokens\n",
    "    * Replace all tokens in the training and testing corpus that are not in the lexicon with an *UNK* token\n",
    "    * Practical approach:\n",
    "        * Lexicon contains all words that appear more than *k* times in the training corpus\n",
    "        * Replace all other tokens with UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b092c",
   "metadata": {},
   "source": [
    "### Unseen Contexts\n",
    "* If the context has not been seen, then calculating probability would also involve dividing by 0\n",
    "* Two basic approaches for dealing with unseen contexts:\n",
    "    * Smoothing / Discounting: move some probability mass from seen trigrams to unseen trigrams\n",
    "    * Back-off: use shorter contexts to fill in gaps, can use bigram or unigram information to approximate trigram information if trigram is unseen (use n-1, n-2, n-k, etc. grams to compute n-gram probability)\n",
    "* Other techniques: \n",
    "    * Class-based backoff, use back-off probability for a specific word class / part-of-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa38920",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "* Problem: n-grams (and most other linguistic phenomena) follow a *Zipfian* distribution\n",
    "* A few words occur very frequently\n",
    "* Most words occur very rarely, and many are seen only once\n",
    "* **Zipf's Law:** a word's frequency is approximately inversely proportional to its rank in the word distribution list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d31735",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "* Smoothing flattens spiky distributions\n",
    "* Take counts from words that have a really high frequency, and distribute among those with lower frequencies to balance/smooth out the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd13dde",
   "metadata": {},
   "source": [
    "### Additive Smoothing\n",
    "* Classic approach: Laplacian, a.k.a. additive smoothing\n",
    "\n",
    "$$ P(w_i) = \\frac{count(w_i) + 1}{N + V} $$\n",
    "\n",
    "where N is the number of tokens, V is the number of types (i.e. the size of the vocabulary)\n",
    "\n",
    "* In the bigram case, we get the following probability:\n",
    "\n",
    "$$ P(w | u) = \\frac{count(u, w) + 1}{count(u) + V} $$\n",
    "\n",
    "* This allows us to avoid the case in which the context, u, has not been seen so we are no longer dividing by zero\n",
    "* However, this treats all unseen words as equal, instead of treating words differently (some words inherently have a higher probability than others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b163fc4",
   "metadata": {},
   "source": [
    "### Linear Interpolation\n",
    "* Use denser distributions of shorter ngrams to \"fill in\" sparse ngram distributions\n",
    "\n",
    "$$ p(w | u, v) = \\lambda_1 \\cdot p_{mle}(w | u, v) + \\lambda_2 \\cdot p_{mle}(w | v) + \\lambda_3 \\cdot p_{mle}(w) $$\n",
    "\n",
    "where $\\lambda_1, \\lambda_2, \\lambda_3 > 0$ and $ \\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\n",
    "\n",
    "* Works well in practice\n",
    "* Parameters can be estimated on development data (for example, using Expectation Maximization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a40863",
   "metadata": {},
   "source": [
    "### Discounting\n",
    "* Idea: set aside some probability mass, then fill in the missing mass using back-off (similar idea to smoothing)\n",
    "\n",
    "$$ count^*(v, w) = count(v, w) - \\beta $$\n",
    "\n",
    "where $0 < \\beta < 1$\n",
    "\n",
    "* Then for all seen bigrams:\n",
    "\n",
    "$$ p(w | v) = \\frac{count^*(v, w)}{count(v)}$$\n",
    "\n",
    "* For each context $v$ the missing probability mass is:\n",
    "\n",
    "$$ \\alpha (v) = 1 - \\sum_{w | c(v, w) > 0} \\frac{count^*(v, w)}{count(v)} $$\n",
    "\n",
    "* We can now divide this held-out mass between the unseen words (evenly, or using back-off)\n",
    "* We can assign this probability mass that has been set aside to the unseen words either uniformly, or distribute it with respect to the unigram count of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a7912",
   "metadata": {},
   "source": [
    "### Katz' Backoff\n",
    "* Divide the held-out probability mass proportionally to the unigram probability of the unseen words in context $v$\n",
    "\n",
    "$$ p(w | v) = \n",
    "\\begin{cases} \\frac{count^*(v, w)}{count(v)} & \\text{if  } count(v, w) > 0\\\\\n",
    "\\alpha (v) \\times \\frac{p_{mle}(w)}{\\sum_{u | c(v, u) = 0} p_{mle}(u) } & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* This allocates the held-out probability mass proportionally to how often the unseen token occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15729723",
   "metadata": {},
   "source": [
    "### Katz' Backoff for Trigrams\n",
    "* For trigrams: recursively compute backoff-probability for unseen bigrams. Then distribute the held-out probability mass proportionally to that bigram backoff-probability.\n",
    "\n",
    "$$ p(w | u, v) = \n",
    "\\begin{cases} \\frac{count^*(u, v, w)}{count(u, v)} & \\text{if } count(u, v, w) > 0 \\\\\n",
    "\\alpha (v) \\times \\frac{p_{BO}(w | v)}{\\sum_{z | c(u, v, w) = 0} p_{BO}(z | v) } & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where from earlier, we have that $\\alpha (v)$:\n",
    "\n",
    "$$ \\alpha (u, v) = 1 - \\sum_{w | count(u, v, w) > 0} \\frac{count^*(u, v, w)}{count(u, v)}\n",
    "$$\n",
    "\n",
    "* Backoff methods are often combined with Good-Turing smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb386a3a",
   "metadata": {},
   "source": [
    "### Evaluating n-gram Models\n",
    "* Extrinsic evaluation: Apply the model in an application (for example, language classification) then evaluate the application\n",
    "* Intrinsic evaluation: measure how well the model approximates unseen language data\n",
    "    * Can compute the probability of each sentence according to the model (higher probability = better model)\n",
    "    * Typically, compute *perplexity* of probability of each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8e80d",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "* **Perplexity -** per word measure of how well the ngram model predicts the sample\n",
    "* Perplexity is defined as $2^{-l}$ where \n",
    "\n",
    "$$l = \\frac{1}{M} \\sum_{i = 1}^{m} \\log_2 p(s_i)$$\n",
    "\n",
    "* Intuitively, perplexity is the average amount of \"surprise\" that the model experiences for every new word, can also be thought of as the \"effective vocabulary size\"\n",
    "* Lower perplexity = better model\n",
    "* Lower perplexity means the model is confident about its predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
