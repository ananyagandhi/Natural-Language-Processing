{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9e4754",
   "metadata": {},
   "source": [
    "# Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630632ae",
   "metadata": {},
   "source": [
    "## Naive Bayes' Classifier and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699a370",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "* Text classification -- use keywords and probabilities to classify text as a specific category\n",
    "    * May be applied to spam detection, mood / sentiment analysis, author identification, identifying political affiliation, word sense disambiguation\n",
    "* Text classification is a machine learning problem -- apply either supervised or unsupervised ML\n",
    "    * Supervised ML: utilize fixed set of classes C, train a classifier from a set of labeled <document, class> pairs\n",
    "        * Discriminative vs. generative models\n",
    "    * Unsupervised ML: unknown set of classes C, topic modeling, utilize clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d23e9d",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "* Requires training data set consisting of labeled data\n",
    "    * Each input vector $x_i$ has some label $y_i$ \n",
    "    * Goal: determine a hypothesis function h(x) that approximates the true relationship between x and y\n",
    "        1. Should be consistent with the training data\n",
    "        2. Model should be generalizable to unseen examples\n",
    "    * Trade-off between (1) and (2) -- increased accuracy with training data causes models to become too specific and less accurate with unseen examples, and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a2dcf",
   "metadata": {},
   "source": [
    "### Representing Documents\n",
    "* To represent the sample text: “to be, or not to be”\n",
    "    * Set-of-words representation: (to, be, or, not)\n",
    "    * Bag-of-words representation: {to: 2, be: 2, or: 1, not: 1}\n",
    "    * Vector-space model: each word corresponds to one dimension in the vector space, entries may be stored as: \n",
    "        * Binary (word appears / does not appear)\n",
    "        * Raw or normalized frequency counts\n",
    "        * Weighted frequency counts\n",
    "        * Probabilities\n",
    "    * Issues / complications with these models: language is ordered, but these models disregard order and associations between words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dc196",
   "metadata": {},
   "source": [
    "### Probabilities in NLP\n",
    "* Ambiguity and uncertainty exists everywhere in NLP\n",
    "* Uncertainty over the “correct interpretation” of speech / text\n",
    "* Utilize probabilities to combine evidence from multiple sources to determine which interpretation of the text is most likely to be correct\n",
    "* Bayesian Statistics: observe some evidence (words in a document) and infer the “correct interpretation” or topic of the text\n",
    "    * Prior probabilities -- probability of an interpretation prior to seeing any evidence\n",
    "    * Conditional (posterior) probability -- probability of an interpretation after taking evidence into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad55309",
   "metadata": {},
   "source": [
    "### Probability Basics\n",
    "* Begin with a sample space $\\Omega$\n",
    "    * Each w in $\\Omega$ is a possible outcome\n",
    "* Probability distributions assign a probability to each basic outcome\n",
    "    * Sum of probabilities of all possible outcomes is 1\n",
    "* Random variable -- function that maps from the outcomes/sample space to the set of real numbers (or to a set of booleans)\n",
    "* Joint probability -- probability of several events occurring\n",
    "\n",
    "$$P(A \\cap B) = P(A, B)$$\n",
    "\n",
    "* Conditional probability -- probability of an event occurring given some knowledge of another event \n",
    "\n",
    "$$P(A | B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A, B)}{P(B)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6024f",
   "metadata": {},
   "source": [
    "### Rules for Conditional Probability\n",
    "* Product Rule:\n",
    "\n",
    "$$ P(A, B) = P(B) \\cdot P(A | B) = P(A) \\cdot P(B | A) $$\n",
    "\n",
    "* Chain Rule (generalization of product rule):\n",
    "\n",
    "$$ \\begin{align}\n",
    "P(A_1, A_2, ... A_n) &= P(A_1 | A_2, A_3, ..., A_n) \\cdot P(A_1, A_2, A_3, ... A_n) \\\\\n",
    "&= P(A_1) \\cdot P(A_2 | A_1) \\cdot ... \\cdot P(A_n | A_1, A_2, ..., A_{n - 1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Bayes' Rule:\n",
    "\n",
    "$$ P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b36c9f",
   "metadata": {},
   "source": [
    "### Independence\n",
    "* Two events are independent if \n",
    "\n",
    "$$ P(A) = P(A | B) $$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$ P(A, B) = P(A) \\cdot P(B) $$\n",
    "\n",
    "* Two events are **conditionally independent** if\n",
    "\n",
    "$$ P(B, C | A) = P(B | A) P(C | A) $$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$ P(B | A, C) = P(B | A) \\text{  and  } P(C | A, B) = P(C | A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6e8f5",
   "metadata": {},
   "source": [
    "### Probabilities and Supervised Learning\n",
    "* Given: training data consisting of training examples\n",
    "\n",
    "$$\n",
    "  \\text{data} = (x_1, y_1), ... , (x_n, y_n)\n",
    "$$\n",
    "\n",
    "Goal: learn a mapping $h$ from x to y. \n",
    "\n",
    "* Learn the mapping using $P(y | x)$\n",
    "* Two approaches:\n",
    "    * Discriminative algorithms learn $P(y | x)$ directly\n",
    "    * Generative algorithms use Bayes Rule:\n",
    "\n",
    "$$ P(y | x) = \\frac{P(x | y) \\cdot P(y)}{P(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65688a8",
   "metadata": {},
   "source": [
    "### Discriminative Algorithms\n",
    "* Model conditional distribution of the label given the data $P(y | x)$\n",
    "* Examples: linear and log-linear models, support vector machine (SVM), decision trees, random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305e7bc",
   "metadata": {},
   "source": [
    "### Generative Algorithms\n",
    "* Assume the observed data is being \"generated\" by a \"hidden\" class label\n",
    "* Build a **different conditional distribution** for each class\n",
    "* Estimate $P(x | y)$ and $P(y)$, then use Bayes Rule:\n",
    "\n",
    "$$ P(y | x) = \\frac{P(x | y) \\cdot P(y)}{P(x)} $$\n",
    "\n",
    "* Examples: Naive Bayes, Hidden Markov Models, Gaussian Mixture Models, PCFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f3a65",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "* Assumption: attributes are all independent of each other, which may not be true in reality\n",
    "\n",
    "$$ P(Label, X_1, ..., X_d) = P(Label) \\prod_i P(X_i | Label) \\\\ \n",
    "   y^* = \\arg \\max_y P(y) \\prod_i P(x_i | y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de29843",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes' Classifier\n",
    "* Goal: use the training data to estimate $P(Label)$ and $P(X_i | Label)$ from training data\n",
    "* Estaimate the prior and posterior probabilities using **Maximum Likelihood Estimates (MLE)**:\n",
    "\n",
    "$$ P(y) = \\frac{Count(y)}{\\sum_{y' \\in Y} Count(y')} \\\\\n",
    "   P(x_i | y) = \\frac{Count(x_i, y)}{\\sum_{x'} Count(x', y)} \n",
    "$$\n",
    "\n",
    "* May face issues if the denominator (or count) is ever 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95d64e",
   "metadata": {},
   "source": [
    "### Independence Assumption\n",
    "* Independence assumption is important, bcause otherwise there would be a lot of joint probabilities involving $X_1, X_2, ..., X_d$\n",
    "* Allows us to estimate each $P(X_i | Label)$ independently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
